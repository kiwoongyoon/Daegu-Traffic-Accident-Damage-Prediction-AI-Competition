{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os \n",
    "import random \n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler,LabelEncoder,OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error,mean_squared_log_error,make_scorer\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import bisect\n",
    "pd.set_option('display.max_columns', None)  # 모든 열을 출력하도록 설정\n",
    "pd.set_option('display.expand_frame_repr', False)  # 너비 제한 없이 출력\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_xtrain_ytrain_xtest(train, test):\n",
    "    x_test = test.drop(columns=['ID']).copy()\n",
    "    y_train =train['ECLO'].copy()\n",
    "    x_train = train[x_test.columns].copy()\n",
    "    return x_train,y_train,x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_function(x_train, x_test, cols_to_onehot) :\n",
    "    xx_train = pd.get_dummies(x_train,columns=cols_to_onehot).copy()\n",
    "    xx_test  = pd.get_dummies(x_test,columns=cols_to_onehot).copy()\n",
    "    \n",
    "    cat_features = []\n",
    "    for col in xx_train.columns:\n",
    "        if xx_train[col].dtype == 'object':\n",
    "            cat_features.append(col)\n",
    "    encoders={}\n",
    "    for feature in cat_features:\n",
    "        le = LabelEncoder()\n",
    "        xx_train[feature] = le.fit_transform(xx_train[feature].astype(str))\n",
    "        le_classes_set = set(le.classes_)\n",
    "        xx_test[feature] = xx_test[feature].map(lambda s: '-1' if s not in le_classes_set else s)\n",
    "        le_classes = le.classes_.tolist()\n",
    "        bisect.insort_left(le_classes, '-1')\n",
    "        le.classes_ = np.array(le_classes)\n",
    "        xx_test[feature] = le.transform(xx_test[feature].astype(str))\n",
    "        encoders[feature] = le\n",
    "        \n",
    "    return xx_train,xx_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test, x_train 생성하기 \n",
    "def evaluate_regr(y,pred):\n",
    "    mae = mean_absolute_error(y,pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y,pred))\n",
    "    log_y = np.log1p(y)\n",
    "    log_pred = np.log1p(pred)\n",
    "    msle = np.mean((log_y - log_pred) ** 2)\n",
    "    rmsle = np.sqrt(msle)    \n",
    "    return rmsle \n",
    "\n",
    "def test_function(x_train,y_train,cols_to_del):\n",
    "    # train.drop(columns=cols_to_del , inplace = True)\n",
    "    # X = train.drop('ECLO',axis=1).values\n",
    "    # y = train['ECLO'].values\n",
    "    # for i in range(0,7):  X[:,i] = LabelEncoder().fit_transform(X[:,i])\n",
    "    # X_standard = StandardScaler().fit_transform(X)\n",
    "    x_train,x_test,y_train,y_test = train_test_split(x_train,y_train,test_size=0.2, random_state= 42 )\n",
    "    gbr = CatBoostRegressor(random_state= 42 ,silent=True)\n",
    "    result_arr = []\n",
    "    \n",
    "    for model in [gbr] :\n",
    "        model.fit(x_train,y_train) \n",
    "        pred = model.predict(x_test)\n",
    "        rmsle =evaluate_regr(y_test, pred) \n",
    "        result_arr.append((rmsle, model))\n",
    "    result_arr.sort(key = lambda x : x[0])\n",
    "    for i in range(len(result_arr)) :\n",
    "        score,model = result_arr[i] \n",
    "        print(f\"제거된 열 :{cols_to_del}의 점수는 {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리  +  특성 생성하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed 고정 및 데이터 불러오기 \n",
    "\n",
    "def seed_everything(seed):\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "seed_everything(42) \n",
    "\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "\n",
    "def data_preprocess(data):\n",
    "    time_pattern = r'(\\d{4})-(\\d{1,2})-(\\d{1,2}) (\\d{1,2})' \n",
    "    data[['연', '월', '일', '시간']] = data['사고일시'].str.extract(time_pattern)\n",
    "    data[['연', '월', '일', '시간']] = data[['연', '월', '일', '시간']].apply(pd.to_numeric) # 추출된 문자열을 수치화해줍니다 \n",
    "    data = data.drop(columns=['사고일시']) # 정보 추출이 완료된 '사고일시' 컬럼은 제거합니다 \n",
    "    location_pattern = r'(\\S+) (\\S+) (\\S+)'\n",
    "    data[['도시', '구', '동']] = data['시군구'].str.extract(location_pattern)\n",
    "    data = data.drop(columns=['시군구'])\n",
    "    road_pattern = r'(.+) - (.+)'\n",
    "    data[['도로형태1', '도로형태2']] = data['도로형태'].str.extract(road_pattern)\n",
    "    data = data.drop(columns=['도로형태'])\n",
    "    data = data.drop(columns=['도시'])\n",
    "    return data \n",
    "\n",
    "train = data_preprocess(train)\n",
    "test = data_preprocess(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이상치 제거하기 \n",
    "to_delete = [10155 ,37536, 32591,12632]\n",
    "train.drop(to_delete, axis=0 ,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간 그룹화 , 휴일 데이터 \n",
    "def make_time_data(data) :\n",
    "    data['요일']= data['요일'].astype(str)\n",
    "    data['weekend'] = np.where(data['요일'].isin(['토요일','일요일']),1,0)\n",
    "    data['time_group'] = data['시간'].apply(convert_hour)\n",
    "    return data \n",
    "\n",
    "def convert_hour(time) :\n",
    "    if time>=0 and  time<6 :\n",
    "        return 0 \n",
    "    elif time>=6 and  time<12 :\n",
    "        return 1\n",
    "    elif time>=12 and  time<18 :\n",
    "        return 2 \n",
    "    else :\n",
    "        return 3\n",
    "train, test  = make_time_data(train) , make_time_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>요일</th>\n",
       "      <th>기상상태</th>\n",
       "      <th>노면상태</th>\n",
       "      <th>연</th>\n",
       "      <th>월</th>\n",
       "      <th>일</th>\n",
       "      <th>시간</th>\n",
       "      <th>동</th>\n",
       "      <th>도로형태1</th>\n",
       "      <th>도로형태2</th>\n",
       "      <th>weekend</th>\n",
       "      <th>time_group</th>\n",
       "      <th>구_남구</th>\n",
       "      <th>구_달서구</th>\n",
       "      <th>구_달성군</th>\n",
       "      <th>구_동구</th>\n",
       "      <th>구_북구</th>\n",
       "      <th>구_서구</th>\n",
       "      <th>구_수성구</th>\n",
       "      <th>구_중구</th>\n",
       "      <th>사고유형_차대사람</th>\n",
       "      <th>사고유형_차대차</th>\n",
       "      <th>사고유형_차량단독</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>79</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>129</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39604</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2021</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>19</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39605</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2021</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>19</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39606</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2021</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>21</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39607</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2021</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>22</td>\n",
       "      <td>158</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39608</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2021</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>89</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39605 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       요일  기상상태  노면상태     연   월   일  시간    동  도로형태1  도로형태2  weekend  time_group  구_남구  구_달서구  구_달성군  구_동구  구_북구  구_서구  구_수성구  구_중구  사고유형_차대사람  사고유형_차대차  사고유형_차량단독\n",
       "0       6     2     0  2019   1   1   0   40      2      5        0           0     0      0      0     0     0     0      0     1          1         0          0\n",
       "1       6     5     0  2019   1   1   0    4      2      5        0           0     0      1      0     0     0     0      0     0          1         0          0\n",
       "2       6     2     0  2019   1   1   1   66      2      5        0           0     0      0      0     0     0     0      1     0          1         0          0\n",
       "3       6     2     0  2019   1   1   2   79      2      5        0           0     0      0      0     0     1     0      0     0          0         1          0\n",
       "4       6     2     0  2019   1   1   4  129      2      5        0           0     0      0      0     1     0     0      0     0          0         1          0\n",
       "...    ..   ...   ...   ...  ..  ..  ..  ...    ...    ...      ...         ...   ...    ...    ...   ...   ...   ...    ...   ...        ...       ...        ...\n",
       "39604   0     2     0  2021  12  31  19  118      0      3        0           3     0      0      0     0     0     0      1     0          0         1          0\n",
       "39605   0     2     0  2021  12  31  19  103      2      5        0           3     0      1      0     0     0     0      0     0          0         1          0\n",
       "39606   0     2     0  2021  12  31  21  144      0      3        0           3     0      1      0     0     0     0      0     0          0         1          0\n",
       "39607   0     2     0  2021  12  31  22  158      1      5        0           3     0      1      0     0     0     0      0     0          0         1          0\n",
       "39608   0     2     0  2021  12  31  23   89      2      8        0           3     0      0      0     0     0     1      0     0          0         1          0\n",
       "\n",
       "[39605 rows x 23 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train,y_train,x_test= make_xtrain_ytrain_xtest(train,test)\n",
    "x_train,x_test = encoding_function(x_train,x_test,['구','사고유형'])\n",
    "# test_function(x_train, y_train, [])\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ECLO의 평균 , 분산을 특성으로 생성하기 \n",
    "# serious는 동으로 혹은 구로만 하는 것 결정 \n",
    "train['serious'] = (1.2*train['사망자수']+train['중상자수']) / (train['부상자수']+ train['경상자수']+0.2)\n",
    "serious_group3 = train.groupby(['구','동','time_group', '사고유형'])['serious'].mean().reset_index().sort_values(by = 'serious').rename(columns={'serious':'serious_type_time'})\n",
    "\n",
    "test = test.merge(serious_group3, on=['구','동','time_group','사고유형'], how='left')\n",
    "train = train.merge(serious_group3, on=['구','동','time_group','사고유형'], how='left')\n",
    "\n",
    "eclo_group_avg = train.groupby(['구','time_group'])['ECLO'].mean().reset_index().sort_values(by = 'ECLO').rename(columns={\"ECLO\":\"eclo_avg1\"})\n",
    "eclo_group_std = train.groupby(['구','time_group'])['ECLO'].std().reset_index().sort_values(by = 'ECLO').rename(columns={\"ECLO\":\"eclo_std1\"})\n",
    "\n",
    "test = test.merge(eclo_group_avg, on=['구','time_group'], how='left')\n",
    "train = train.merge(eclo_group_avg,on=['구','time_group'], how='left')\n",
    "test = test.merge(eclo_group_std, on=['구','time_group'], how='left')\n",
    "train = train.merge(eclo_group_std,on=['구','time_group'], how='left')\n",
    "\n",
    "eclo_group_avg2 = train.groupby(['구','time_group', '요일'])['ECLO'].mean().reset_index().sort_values(by = 'ECLO').rename(columns={\"ECLO\":\"eclo_avg2\"})\n",
    "eclo_group_std2 = train.groupby(['구','time_group','요일'])['ECLO'].std().reset_index().sort_values(by = 'ECLO').rename(columns={\"ECLO\":\"eclo_std2\"})\n",
    "\n",
    "test = test.merge(eclo_group_avg2, on=['구','time_group','요일'], how='left')\n",
    "train = train.merge(eclo_group_avg2,on=['구','time_group','요일'], how='left')\n",
    "test = test.merge(eclo_group_std2, on=['구','time_group','요일'], how='left')\n",
    "train = train.merge(eclo_group_std2,on=['구','time_group','요일'], how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 외부 데이터 추출하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 외부데이터 (보안등, 어린이 보호구역 , 대구 주차장 , cctv )\n",
    "light_df = pd.read_csv('./data/external_open/대구 보안등 정보.csv', encoding='cp949')[['설치개수', '소재지지번주소']]\n",
    "location_pattern = r'(\\S+) (\\S+) (\\S+) (\\S+)'\n",
    "light_df[['도시', '구', '동', '번지']] = light_df['소재지지번주소'].str.extract(location_pattern)\n",
    "light_df = light_df.drop(columns=['소재지지번주소', '번지'])\n",
    "light_df = light_df.groupby(['도시', '구', '동']).sum().reset_index()\n",
    "light_df.reset_index(inplace=True, drop=True)\n",
    "# light_df\n",
    "\n",
    "#어린이 보호 구역 정보\n",
    "child_area_df = pd.read_csv('./data/external_open/대구 어린이 보호 구역 정보.csv', encoding='cp949').drop_duplicates()[['소재지지번주소']]\n",
    "child_area_df['kid_area'] = 1\n",
    "\n",
    "child_area_df[['도시', '구', '동', '번지']] = child_area_df['소재지지번주소'].str.extract(location_pattern)\n",
    "child_area_df = child_area_df.drop(columns=['소재지지번주소', '번지'])\n",
    "\n",
    "child_area_df = child_area_df.groupby(['도시', '구', '동']).sum().reset_index()\n",
    "child_area_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# 대구 주차장 정보\n",
    "parking_df = pd.read_csv('./data/external_open/대구 주차장 정보.csv', encoding='cp949')[['소재지지번주소', '급지구분','주차구획수']]\n",
    "parking_df = pd.get_dummies(parking_df, columns=['급지구분'])\n",
    "\n",
    "parking_df[['도시', '구', '동', '번지']] = parking_df['소재지지번주소'].str.extract(location_pattern)\n",
    "parking_df = parking_df.drop(columns=['소재지지번주소', '번지'])\n",
    "\n",
    "parking_df = parking_df.groupby(['도시', '구', '동']).sum().reset_index()\n",
    "parking_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# 대구 cctv정보 \n",
    "cctv_df = pd.read_csv('./data/external_open/대구 CCTV 정보.csv', encoding='cp949')\n",
    "cctv_df['cctv_cnt'] = 1 \n",
    "# location_pattern = r'(\\S+) (\\S+) (\\S+) (\\S+)'\n",
    "cctv_df[['도시', '구', '동', '번지']] = cctv_df['소재지지번주소'].str.extract(location_pattern)\n",
    "cctv_df = cctv_df.drop(columns=['소재지지번주소', '번지'])\n",
    "\n",
    "cctv_df = cctv_df.groupby(['도시', '구', '동']).sum().reset_index()\n",
    "cctv_df.reset_index(inplace=True, drop=True)\n",
    "cctv_df = cctv_df[['도시', '구', '동','cctv_cnt','제한속도']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for one_data in [light_df, child_area_df,parking_df, cctv_df ]:\n",
    "    test = test.merge(one_data,on = ['도시', '구', '동'],how = 'left')\n",
    "    train= train.merge(one_data,on = ['도시', '구', '동'],how = 'left')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 보행자 사고 데이터 \n",
    "walker_df = pd.read_csv('./data/보행자사고지표.csv')\n",
    "walker_df = walker_df.drop(columns=['치사율', '지점'])\n",
    "walker_df['eclo_walker'] = (1.5*walker_df['사망'] +1.2*walker_df['중상'])/(1+ 1*walker_df['경상']+walker_df['부상']) \n",
    "walker_df = walker_df.groupby(['구','동']).median().reset_index()\n",
    "walker_df['eclo_walker_avg'] = walker_df['eclo_walker']/walker_df['건수']\n",
    "walker_df = walker_df.sort_values(by = 'eclo_walker_avg')\n",
    "walker_df = walker_df[['구', '동', 'eclo_walker_avg']]\n",
    "walker_df.tail(10)\n",
    "\n",
    "# 노약자 사고 데이터 \n",
    "olders_df = pd.read_csv('./data/노약자사고지표.csv').drop(columns=['지역'])\n",
    "olders_df['eclo_older']= 10*olders_df['사망'] +5*olders_df['중상']+3*olders_df['경상']\n",
    "olders_df = olders_df.groupby(['구','동']).median().reset_index()\n",
    "olders_df['eclo_older_avg'] = olders_df['eclo_older']/olders_df['건수']\n",
    "olders_df =olders_df.sort_values(by = 'eclo_older_avg')\n",
    "olders_df = olders_df[['구', '동', 'eclo_older_avg']]\n",
    "olders_df.tail(12)\n",
    "\n",
    "for one_data in [walker_df , olders_df] :\n",
    "    test = test.merge(one_data,on=['구', '동'], how = 'left')\n",
    "    train = train.merge(one_data ,on =['구', '동'],how = 'left')\n",
    "    \n",
    "    \n",
    "# for col in ['eclo_walker_avg', 'eclo_older_avg'] : \n",
    "#     test[col] = test[col].fillna(0)\n",
    "#     train[col] = train[col].fillna(0)\n",
    "\n",
    "# # df['a'].fillna(df.groupby('d')['a'].transform('mean'), inplace=True)\n",
    "# for col in ['주차구획수','급지구분_1','급지구분_2','급지구분_3','cctv_cnt','제한속도']:\n",
    "#     test[col] = test[col].fillna(test.groupby('구')[col].transform('mean'))\n",
    "#     train[col] = train[col].fillna(train.groupby('구')[col].transform('mean'))\n",
    "\n",
    "# test['serious_type_time'] = test['serious_type_time'].fillna(test.groupby('구')['serious_type_time'].transform('mean'))\n",
    "\n",
    "# for col in ['설치개수','kid_area']:\n",
    "#     test[col] = test[col].fillna(test[col].mean())\n",
    "#     train[col] = train[col].fillna(train[col].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_cnt =pd.read_csv('./data/대구광역시_주민등록인구 통계현황.csv',encoding='cp949')\n",
    "car_cnt = pd.read_csv('./data/대구광역시_읍면동별 자동차 등록현황_20211031.csv',encoding='cp949')\n",
    "olders = pd.read_csv('./data/노인요양시설.csv', encoding='cp949')\n",
    "\n",
    "# 구별 인구 데이터 생성 \n",
    "population_cnt.drop(population_cnt.iloc[0].name, inplace = True )\n",
    "population_cnt['구'] = population_cnt['행정구역'].astype(str).str.slice(6,9)\n",
    "population_cnt['구'] = population_cnt['구'].str.strip()\n",
    "population_cnt = population_cnt[['구','2021년05월_총인구수','2021년05월_남자 인구수']]\n",
    "\n",
    "train = train.merge(population_cnt , on ='구',how = 'left')\n",
    "test = test.merge(population_cnt , on ='구',how = 'left')\n",
    "\n",
    "#구별 노인 수용 가능 인원 데이터 \n",
    "olders_grouped = olders.groupby('구분')['입소정원'].agg(['sum']).reset_index()\n",
    "olders_grouped=olders_grouped.rename(columns={'구분': '구', 'sum':'노인수용가능'})\n",
    "\n",
    "train = train.merge(olders_grouped , on = '구',how ='left')\n",
    "test = test.merge(olders_grouped , on = '구',how ='left')\n",
    "\n",
    "#지역별 등록된 자동차 수 \n",
    "car_cnt = car_cnt.rename(columns={'구군':'구', '읍면동':'동'})\n",
    "train = train.merge(car_cnt, on =['구', '동'],how='left')\n",
    "test = test.merge(car_cnt, on =['구', '동'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test.drop(columns=['ID']).copy()\n",
    "y_train =train['ECLO'].copy()\n",
    "x_train = train[x_test.columns].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = []\n",
    "for col in x_train.columns:\n",
    "    if x_train[col].dtype == 'object':\n",
    "        cat_features.append(col)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect\n",
    "encoders={}\n",
    "for feature in cat_features:\n",
    "    le = LabelEncoder()\n",
    "    x_train[feature] = le.fit_transform(x_train[feature].astype(str))\n",
    "    le_classes_set = set(le.classes_)\n",
    "    x_test[feature] = x_test[feature].map(lambda s: '-1' if s not in le_classes_set else s)\n",
    "    le_classes = le.classes_.tolist()\n",
    "    bisect.insort_left(le_classes, '-1')\n",
    "    le.classes_ = np.array(le_classes)\n",
    "    x_test[feature] = le.transform(x_test[feature].astype(str))\n",
    "    encoders[feature] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(x_train)\n",
    "test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.DataFrame(x_train, columns = x_train.columns)\n",
    "x_test = pd.DataFrame(x_test, columns = x_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from supervised.automl import AutoML\n",
    "\n",
    "def rmsle_metric(true, predicted, sample_weight =None) :\n",
    "    true = np.array(true)\n",
    "    predicted = np.array(predicted) \n",
    "    log_true = np.log1p(true)\n",
    "    log_predicted = np.log1p(predicted) \n",
    "    difference = log_predicted-log_true \n",
    "    difference = np.square(difference) \n",
    "    mean_diff = difference.mean() \n",
    "    score = np.sqrt(mean_diff) \n",
    "    return score \n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "def rmsle(y_true, y_predicted, sample_weight=None):\n",
    "    val = mean_squared_log_error(y_true, y_predicted, sample_weight=sample_weight)\n",
    "    return np.sqrt(val) if val > 0 else -np.Inf\n",
    "\n",
    "automl = AutoML(\n",
    "    \n",
    "    mode=\"Compete\",\n",
    "    ml_task=\"regression\",\n",
    "    eval_metric='rmse',\n",
    "    random_state=42,\n",
    "    total_time_limit=60*60*2,\n",
    "    model_time_limit=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear algorithm was disabled.\n",
      "AutoML directory: AutoML_2\n",
      "The task is regression with evaluation metric rmse\n",
      "AutoML will use algorithms: ['Decision Tree', 'Random Forest', 'Extra Trees', 'LightGBM', 'Xgboost', 'CatBoost', 'Neural Network', 'Nearest Neighbors']\n",
      "AutoML will stack models\n",
      "AutoML will ensemble available models\n",
      "AutoML steps: ['adjust_validation', 'simple_algorithms', 'default_algorithms', 'not_so_random', 'mix_encoding', 'golden_features', 'kmeans_features', 'insert_random_feature', 'features_selection', 'hill_climbing_1', 'hill_climbing_2', 'boost_on_errors', 'ensemble', 'stack', 'ensemble_stacked']\n",
      "* Step adjust_validation will try to check up to 1 model\n",
      "1_DecisionTree rmse 3.037163 trained in 0.5 seconds\n",
      "Adjust validation. Remove: 1_DecisionTree\n",
      "Validation strategy: 10-fold CV Shuffle\n",
      "* Step simple_algorithms will try to check up to 3 models\n",
      "1_DecisionTree rmse 3.095144 trained in 4.04 seconds\n",
      "2_DecisionTree rmse 3.107493 trained in 3.66 seconds\n",
      "3_DecisionTree rmse 3.107493 trained in 3.75 seconds\n",
      "* Step default_algorithms will try to check up to 7 models\n",
      "4_Default_LightGBM rmse 3.166326 trained in 23.78 seconds\n",
      "5_Default_Xgboost rmse 3.16058 trained in 5.52 seconds\n",
      "6_Default_CatBoost rmse 3.159963 trained in 20.49 seconds\n",
      "7_Default_NeuralNetwork rmse 3.194436 trained in 24.81 seconds\n",
      "8_Default_RandomForest rmse 3.173507 trained in 19.47 seconds\n",
      "9_Default_ExtraTrees rmse 3.179914 trained in 10.26 seconds\n",
      "There was an error during 10_Default_NearestNeighbors training.\n",
      "Please check AutoML_2/errors.md for details.\n",
      "* Step not_so_random will try to check up to 63 models\n",
      "19_LightGBM rmse 3.168944 trained in 20.72 seconds\n",
      "10_Xgboost rmse 3.161484 trained in 5.27 seconds\n",
      "28_CatBoost rmse 3.16257 trained in 15.45 seconds\n",
      "37_RandomForest rmse 3.172687 trained in 19.27 seconds\n",
      "46_ExtraTrees rmse 3.179178 trained in 19.13 seconds\n",
      "55_NeuralNetwork rmse 3.233836 trained in 14.12 seconds\n",
      "There was an error during 64_NearestNeighbors training.\n",
      "Please check AutoML_2/errors.md for details.\n",
      "20_LightGBM rmse 3.163835 trained in 20.12 seconds\n",
      "11_Xgboost rmse 3.165023 trained in 5.7 seconds\n",
      "29_CatBoost rmse 3.160546 trained in 11.7 seconds\n",
      "38_RandomForest rmse 3.170951 trained in 12.6 seconds\n",
      "47_ExtraTrees rmse 3.178459 trained in 10.49 seconds\n",
      "56_NeuralNetwork rmse 3.205072 trained in 24.55 seconds\n",
      "There was an error during 65_NearestNeighbors training.\n",
      "Please check AutoML_2/errors.md for details.\n",
      "21_LightGBM rmse 3.162947 trained in 363.0 seconds\n",
      "12_Xgboost rmse 3.158847 trained in 11.14 seconds\n",
      "30_CatBoost rmse 3.400564 trained in 422.71 seconds\n",
      "39_RandomForest rmse 3.182912 trained in 13.49 seconds\n",
      "48_ExtraTrees rmse 3.191776 trained in 9.6 seconds\n",
      "57_NeuralNetwork rmse 3.171668 trained in 16.64 seconds\n",
      "There was an error during 66_NearestNeighbors training.\n",
      "Please check AutoML_2/errors.md for details.\n",
      "22_LightGBM rmse 3.167987 trained in 12.05 seconds\n",
      "13_Xgboost rmse 3.164564 trained in 6.5 seconds\n",
      "31_CatBoost rmse 3.494142 trained in 33.07 seconds\n",
      "40_RandomForest rmse 3.174768 trained in 12.48 seconds\n",
      "49_ExtraTrees rmse 3.181108 trained in 9.83 seconds\n",
      "58_NeuralNetwork rmse 3.174349 trained in 11.4 seconds\n",
      "There was an error during 67_NearestNeighbors training.\n",
      "Please check AutoML_2/errors.md for details.\n",
      "23_LightGBM rmse 3.166234 trained in 33.2 seconds\n",
      "14_Xgboost rmse 3.162593 trained in 6.8 seconds\n",
      "32_CatBoost rmse 3.16142 trained in 34.7 seconds\n",
      "41_RandomForest rmse 3.183677 trained in 14.28 seconds\n",
      "50_ExtraTrees rmse 3.190717 trained in 8.23 seconds\n",
      "59_NeuralNetwork rmse 3.198051 trained in 15.52 seconds\n",
      "There was an error during 68_NearestNeighbors training.\n",
      "Please check AutoML_2/errors.md for details.\n",
      "24_LightGBM rmse 3.167095 trained in 33.18 seconds\n",
      "15_Xgboost rmse 3.16273 trained in 6.27 seconds\n",
      "33_CatBoost rmse 3.455566 trained in 81.47 seconds\n",
      "42_RandomForest rmse 3.176593 trained in 14.59 seconds\n",
      "51_ExtraTrees rmse 3.17992 trained in 9.12 seconds\n",
      "60_NeuralNetwork rmse 3.225153 trained in 34.06 seconds\n",
      "There was an error during 69_NearestNeighbors training.\n",
      "Please check AutoML_2/errors.md for details.\n",
      "25_LightGBM rmse 3.1715 trained in 43.37 seconds\n",
      "16_Xgboost rmse 3.157737 trained in 7.11 seconds\n",
      "34_CatBoost rmse 3.408897 trained in 36.33 seconds\n",
      "43_RandomForest rmse 3.168914 trained in 14.31 seconds\n",
      "52_ExtraTrees rmse 3.173729 trained in 14.05 seconds\n",
      "61_NeuralNetwork rmse 3.193687 trained in 15.82 seconds\n",
      "There was an error during 70_NearestNeighbors training.\n",
      "Please check AutoML_2/errors.md for details.\n",
      "26_LightGBM rmse 3.169557 trained in 22.1 seconds\n",
      "17_Xgboost rmse 3.162597 trained in 7.06 seconds\n",
      "35_CatBoost rmse 3.160105 trained in 15.34 seconds\n",
      "44_RandomForest rmse 3.181209 trained in 11.48 seconds\n",
      "53_ExtraTrees rmse 3.188061 trained in 10.45 seconds\n",
      "62_NeuralNetwork rmse 3.207567 trained in 17.32 seconds\n",
      "There was an error during 71_NearestNeighbors training.\n",
      "Please check AutoML_2/errors.md for details.\n",
      "27_LightGBM rmse 3.170138 trained in 1472.8 seconds\n",
      "18_Xgboost rmse 3.162602 trained in 17.18 seconds\n",
      "36_CatBoost rmse 3.454441 trained in 301.78 seconds\n",
      "* Step mix_encoding will try to check up to 1 model\n",
      "16_Xgboost_categorical_mix rmse 3.154079 trained in 32.53 seconds\n",
      "* Step golden_features will try to check up to 3 models\n",
      "None 10\n",
      "Add Golden Feature: eclo_avg2_multiply_연\n",
      "Add Golden Feature: 소계_ratio_weekend\n",
      "Add Golden Feature: 소계_multiply_weekend\n",
      "Add Golden Feature: weekend_ratio_소계\n",
      "Add Golden Feature: serious_type_time_diff_eclo_avg2\n",
      "Add Golden Feature: serious_type_time_multiply_월\n",
      "Add Golden Feature: 2021년05월_총인구수_sum_serious_type_time\n",
      "Add Golden Feature: 주차구획수_multiply_serious_type_time\n",
      "Add Golden Feature: 소계_multiply_serious_type_time\n",
      "Add Golden Feature: serious_type_time_ratio_eclo_std2\n",
      "Created 10 Golden Features in 6.09 seconds.\n",
      "1_DecisionTree_GoldenFeatures rmse 3.095076 trained in 13.92 seconds\n",
      "2_DecisionTree_GoldenFeatures rmse 3.10685 trained in 7.49 seconds\n",
      "3_DecisionTree_GoldenFeatures rmse 3.10685 trained in 6.7 seconds\n",
      "* Step kmeans_features will try to check up to 3 models\n",
      "1_DecisionTree_KMeansFeatures rmse 3.096981 trained in 77.7 seconds\n",
      "2_DecisionTree_KMeansFeatures rmse 3.107493 trained in 88.03 seconds\n",
      "3_DecisionTree_KMeansFeatures rmse 3.107493 trained in 88.26 seconds\n",
      "* Step insert_random_feature will try to check up to 1 model\n",
      "1_DecisionTree_GoldenFeatures_RandomFeature rmse 3.095274 trained in 12.77 seconds\n",
      "Drop features ['serious_type_time_ratio_eclo_std2', 'eclo_avg2', '승합', 'serious_type_time_multiply_월', '소계', 'random_feature', 'eclo_walker_avg', '설치개수', 'eclo_std2', '일', 'eclo_std1', 'eclo_avg1', 'weekend', 'time_group', '주차구획수', '도로형태1', '동', '구', '시간', '요일', '소계_multiply_weekend', '연', '제한속도', 'eclo_older_avg', '2021년05월_총인구수', '2021년05월_남자 인구수', '노인수용가능', '화물', '급지구분_3', '특수', '소계_ratio_weekend', '노면상태', 'weekend_ratio_소계', '2021년05월_총인구수_sum_serious_type_time', '주차구획수_multiply_serious_type_time', '급지구분_1', '급지구분_2', 'kid_area', '기상상태', 'cctv_cnt', '월', 'serious_type_time_diff_eclo_avg2']\n",
      "* Step features_selection will try to check up to 6 models\n",
      "16_Xgboost_categorical_mix_SelectedFeatures rmse 3.169438 trained in 21.32 seconds\n",
      "6_Default_CatBoost_SelectedFeatures rmse 3.18 trained in 14.23 seconds\n",
      "21_LightGBM_SelectedFeatures rmse 3.179882 trained in 80.82 seconds\n",
      "43_RandomForest_SelectedFeatures rmse 3.182574 trained in 13.34 seconds\n",
      "57_NeuralNetwork_SelectedFeatures rmse 3.183203 trained in 22.47 seconds\n",
      "52_ExtraTrees_SelectedFeatures rmse 3.186555 trained in 9.14 seconds\n",
      "* Step hill_climbing_1 will try to check up to 30 models\n",
      "63_DecisionTree_GoldenFeatures rmse 3.099922 trained in 6.68 seconds\n",
      "64_DecisionTree rmse 3.100629 trained in 6.29 seconds\n",
      "65_DecisionTree rmse 3.100737 trained in 84.63 seconds\n",
      "66_Xgboost rmse 3.154905 trained in 29.59 seconds\n",
      "67_Xgboost rmse 3.158108 trained in 30.02 seconds\n",
      "68_Xgboost rmse 3.157071 trained in 30.38 seconds\n",
      "69_CatBoost rmse 3.357486 trained in 61.77 seconds\n",
      "70_CatBoost rmse 3.343621 trained in 90.42 seconds\n",
      "71_CatBoost rmse 3.356077 trained in 26.43 seconds\n",
      "72_LightGBM rmse 3.16229 trained in 760.66 seconds\n",
      "73_LightGBM rmse 3.165429 trained in 90.91 seconds\n",
      "74_LightGBM rmse 3.163728 trained in 31.47 seconds\n",
      "75_LightGBM rmse 3.162758 trained in 93.03 seconds\n",
      "* Step hill_climbing_2 will try to check up to 27 models\n",
      "76_Xgboost rmse 3.155344 trained in 13.82 seconds\n",
      "77_Xgboost rmse 3.156618 trained in 30.12 seconds\n",
      "78_Xgboost rmse 3.158524 trained in 29.08 seconds\n",
      "79_Xgboost rmse 3.156202 trained in 27.25 seconds\n",
      "80_Xgboost rmse 3.156699 trained in 30.54 seconds\n",
      "81_Xgboost rmse 3.158683 trained in 38.4 seconds\n",
      "82_LightGBM not trained. Stop training after the first fold. Time needed to train on the first fold 258.0 seconds. The time estimate for training on all folds is larger than total_time_limit.\n",
      "83_LightGBM not trained. Stop training after the first fold. Time needed to train on the first fold 259.0 seconds. The time estimate for training on all folds is larger than total_time_limit.\n",
      "* Step boost_on_errors will try to check up to 1 model\n",
      "1_DecisionTree_GoldenFeatures_BoostOnErrors rmse 3.103159 trained in 8.32 seconds\n",
      "* Step ensemble will try to check up to 1 model\n",
      "Ensemble rmse 3.090138 trained in 9.92 seconds\n",
      "* Step stack will try to check up to 60 models\n",
      "16_Xgboost_categorical_mix_Stacked rmse 3.153334 trained in 46.63 seconds\n",
      "6_Default_CatBoost_Stacked rmse 3.160767 trained in 34.23 seconds\n",
      "72_LightGBM_Stacked not trained. Stop training after the first fold. Time needed to train on the first fold 230.0 seconds. The time estimate for training on all folds is larger than total_time_limit.\n",
      "43_RandomForest_Stacked rmse 3.158165 trained in 148.29 seconds\n",
      "57_NeuralNetwork_Stacked rmse 3.167246 trained in 102.03 seconds\n",
      "52_ExtraTrees_Stacked rmse 3.158621 trained in 26.0 seconds\n",
      "66_Xgboost_Stacked rmse 3.149919 trained in 20.68 seconds\n",
      "35_CatBoost_Stacked rmse 3.162773 trained in 37.64 seconds\n",
      "75_LightGBM_Stacked not trained. Stop training after the first fold. Time needed to train on the first fold 243.0 seconds. The time estimate for training on all folds is larger than total_time_limit.\n",
      "38_RandomForest_Stacked rmse 3.157119 trained in 64.87 seconds\n",
      "58_NeuralNetwork_Stacked rmse 3.1606 trained in 26.53 seconds\n",
      "47_ExtraTrees_Stacked rmse 3.159188 trained in 20.12 seconds\n",
      "76_Xgboost_Stacked rmse 3.149618 trained in 13.08 seconds\n",
      "29_CatBoost_Stacked rmse 3.159667 trained in 12.61 seconds\n",
      "21_LightGBM_Stacked not trained. Stop training after the first fold. Time needed to train on the first fold 57.0 seconds. The time estimate for training on all folds is larger than total_time_limit.\n",
      "37_RandomForest_Stacked not trained. Stop training after the first fold. Time needed to train on the first fold 5.0 seconds. The time estimate for training on all folds is larger than total_time_limit.\n",
      "* Step ensemble_stacked will try to check up to 1 model\n",
      "Ensemble_Stacked rmse 3.089216 trained in 10.56 seconds\n",
      "AutoML fit time: 7244.1 seconds\n",
      "AutoML best model: Ensemble_Stacked\n"
     ]
    }
   ],
   "source": [
    "automl.fit(x_train, y_train)\n",
    "pred = automl.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub= pd.read_csv('./data/sample_submission.csv')\n",
    "sub['ECLO']=pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('./data/submission/mljar_newstart.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
